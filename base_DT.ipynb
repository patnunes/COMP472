{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#imports for DT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base-DT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the decision tree models created, the second model seems to have much better predictions, and overall has better performance in terms of recall, precision, f1-measure. This is most likely due to the high number of classes in DS1, as it has 26 classes in comparison to DS2, that only has 9. The decision tree will grow with a much higher complexity for DS1 because of the possible leaf nodes. Another thing to note is the training set for DS2 (7800) has much more instance in comparison to DS1 (1197). Which could allow for the training model to make predictons overall, based on the available instances.\n",
    "\n",
    "Data Set 1: \n",
    "Based on the confusion matrix, the max the model misclassified 2 or lower. However, the avg recall and avg precision is fairly low (50%), therefore very few of the model's positive predictions are true and many of the positive values are never predicted.\n",
    "\n",
    "Data Set 2:\n",
    "The model predicted many of the labels correctly except for 5 & 9 as they had the most miclassifications. Both the avg recall and avg precision were fairly high (~80%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve test data for dataset1 from csv file\n",
    "test_set1 = pd.read_csv(\"Assig1-Dataset/test_with_label_1.csv\", header=None)\n",
    "#test_set1.shape #display number of (rows, columns) to ensure all the data was retrieved\n",
    "\n",
    "#seperate values/labels from the test data\n",
    "y_test1 = test_set1[1024] #the set of labels to all the data in x_test\n",
    "x_test1 = test_set1.drop(1024, axis=1) #test data set\n",
    "\n",
    "#Retrieve train set data for dataset1 from csv file\n",
    "train_set1 = pd.read_csv(\"Assig1-Dataset/train_1.csv\", header=None)\n",
    "\n",
    "#seperate values/labels from the train data\n",
    "target_train1 = train_set1[1024] #y_train, the set of labels to all the data in x_train\n",
    "x_train1 = train_set1.drop(1024,axis=1) #the training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for ds1\n",
    "dtree1 = DecisionTreeClassifier(criterion = \"entropy\") #Use entropy as the decision criterion for the tree, all other values are set to default\n",
    "dtree1 = dtree1.fit(x_train1, target_train1) #fit is used to train the algo based on the training data\n",
    "dt_pred1 = dtree1.predict(x_test1) #predictions based on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#print confustion matrix\n",
    "print(confusion_matrix(y_test1, dt_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report: Precision, Recall, F-1 Measure for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.60      0.75      0.67         4\n",
      "           3       0.33      0.33      0.33         3\n",
      "           4       0.50      0.50      0.50         2\n",
      "           5       0.33      0.50      0.40         2\n",
      "           6       0.25      0.25      0.25         4\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       1.00      0.33      0.50         3\n",
      "           9       0.75      0.75      0.75         4\n",
      "          10       0.00      0.00      0.00         3\n",
      "          11       1.00      0.75      0.86         4\n",
      "          12       0.00      0.00      0.00         3\n",
      "          13       0.60      0.75      0.67         4\n",
      "          14       0.40      0.67      0.50         3\n",
      "          15       0.50      1.00      0.67         3\n",
      "          16       0.50      0.33      0.40         3\n",
      "          17       1.00      0.50      0.67         4\n",
      "          18       1.00      0.67      0.80         3\n",
      "          19       0.50      0.50      0.50         2\n",
      "          20       0.50      1.00      0.67         3\n",
      "          21       0.40      0.67      0.50         3\n",
      "          22       0.00      0.00      0.00         3\n",
      "          23       0.50      0.50      0.50         2\n",
      "          24       0.40      0.67      0.50         3\n",
      "          25       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.50        80\n",
      "   macro avg       0.48      0.48      0.45        80\n",
      "weighted avg       0.51      0.50      0.47        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print classificaton report\n",
    "print(classification_report(y_test1, dt_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve test data for dataset2 from csv file\n",
    "test_set2 = pd.read_csv(\"Assig1-Dataset/test_with_label_2.csv\", header=None)\n",
    "\n",
    "#seperate values/labels from the test data\n",
    "y_test2 = test_set2[1024] #the set of labels to all the data in x_test\n",
    "x_test2 = test_set2.drop(1024, axis=1) #test data set\n",
    "\n",
    "#Retrieve training data for dataset2 from csv file\n",
    "train_set2 = pd.read_csv(\"Assig1-Dataset/train_2.csv\", header=None)\n",
    "\n",
    "#seperate values/labels from the test data\n",
    "target_train2 = train_set2[1024] #y_train, the set of labels to all the data in x_train\n",
    "x_train2 = train_set2.drop(1024,axis=1) #the training data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for ds2\n",
    "dtree2 = DecisionTreeClassifier(criterion = \"entropy\") #Use entropy as the decision criterion for the tree, all other values are set to default\n",
    "dtree2 = dtree2.fit(x_train2, target_train2) #fit is used to train the algo based on the training data\n",
    "dt_pred2 = dtree2.predict(x_test2) #predictions based on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 43   0   1   0   7   0   0   0   1   3]\n",
      " [  1 114   0   5   0   2   1   0   2   0]\n",
      " [  0   0   7   0   1   2   0   0   3   2]\n",
      " [  0   3   0  10   0   1   0   1   0   0]\n",
      " [  3   1   0   2  27   2   1   0   1  13]\n",
      " [  0   0   1   0   5  42   0   0   0   7]\n",
      " [  0   1   2   0   1   0   7   0   2   2]\n",
      " [  0   1   0   0   0   0   0  13   1   0]\n",
      " [  4   1   0   0   1   0   0   0  43   1]\n",
      " [  1   2   1   0   9  14   0   0   1  97]]\n"
     ]
    }
   ],
   "source": [
    "#print out confusion matrix\n",
    "print(confusion_matrix(y_test2, dt_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report: Precision, Recall, F-1 Measure for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80        55\n",
      "           1       0.93      0.91      0.92       125\n",
      "           2       0.58      0.47      0.52        15\n",
      "           3       0.59      0.67      0.62        15\n",
      "           4       0.53      0.54      0.53        50\n",
      "           5       0.67      0.76      0.71        55\n",
      "           6       0.78      0.47      0.58        15\n",
      "           7       0.93      0.87      0.90        15\n",
      "           8       0.80      0.86      0.83        50\n",
      "           9       0.78      0.78      0.78       125\n",
      "\n",
      "    accuracy                           0.78       520\n",
      "   macro avg       0.74      0.71      0.72       520\n",
      "weighted avg       0.78      0.78      0.77       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(classification_report(y_test2, dt_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Results to CSV for both data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSV file Base-DT-DS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file Base-DT-DS1 created!\n"
     ]
    }
   ],
   "source": [
    "filename = './outputs/Base-DT-DS1.csv'\n",
    "\n",
    "Base_DT_DS1 = pd.DataFrame(data=dt_pred1)\n",
    "Base_DT_DS1.index = Base_DT_DS1.index + 1\n",
    "Base_DT_DS1.to_csv(filename, header=None)\n",
    "\n",
    "writer = open(filename, 'a')\n",
    "\n",
    "writer.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "writer.write(str(confusion_matrix(y_test1, dt_pred1)))\n",
    "writer.write(\"\\n\\n\\n\")\n",
    "writer.write(\"\\nClassification Report:\\n\\n\")\n",
    "writer.write(str(classification_report(y_test1, dt_pred1)))\n",
    "\n",
    "writer.close()\n",
    "print(\"Output file Base-DT-DS1 created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSV file Base-DT-DS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file Base-DT-DS2 created!\n"
     ]
    }
   ],
   "source": [
    "filename = './outputs/Base-DT-DS2.csv'\n",
    "\n",
    "Base_DT_DS2 = pd.DataFrame(dt_pred2)\n",
    "Base_DT_DS2.index = Base_DT_DS2.index + 1\n",
    "Base_DT_DS2.to_csv(filename, header=None)\n",
    "\n",
    "writer = open(filename, 'a')\n",
    "\n",
    "writer.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "writer.write(str(confusion_matrix(y_test2, dt_pred2)))\n",
    "writer.write(\"\\n\\n\\n\")\n",
    "writer.write(\"\\nClassification Report:\\n\\n\")\n",
    "writer.write(str(classification_report(y_test2, dt_pred2)))\n",
    "\n",
    "writer.close()\n",
    "print(\"Output file Base-DT-DS2 created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
